# 어텐션 블록 정의
def attention_block(x, g, inter_channel):
    theta_x = Conv2D(inter_channel, (2, 2), strides=(2, 2), padding='same')(x)  # Theta 연산
    phi_g = Conv2D(inter_channel, (1, 1), padding='same')(g)  # Phi 연산
    f = Activation('relu')(add([theta_x, phi_g]))  # Theta(x) + Phi(g) 후 활성화 함수(Relu)
    psi_f = Conv2D(1, (1, 1), padding='same')(f)  # Psi 연산
    rate = Activation('sigmoid')(psi_f)  # 시그모이드 활성화 함수
    upsampled_rate = UpSampling2D(size=(2, 2))(rate)  # UpSampling2D를 사용하여 rate의 크기를 x와 동일하게 조정
    attended = Multiply()([x, upsampled_rate])  # x와 가중치를 곱하여 어텐션 적용
    return attended

# 함수 형태로 모델을 정의
def f_model(class_weights_array):
    input_shape = (240, 240, 4)
    inputs = Input(shape=input_shape)

    # Encoder
    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    conv1 = BatchNormalization()(conv1)
    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)
    conv1 = BatchNormalization()(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)
    conv2 = BatchNormalization()(conv2)
    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)
    conv2 = BatchNormalization()(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)
    conv3 = BatchNormalization()(conv3)
    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)
    conv3 = BatchNormalization()(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)
    conv4 = BatchNormalization()(conv4)
    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)
    conv4 = BatchNormalization()(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

    conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(pool4)
    conv5 = BatchNormalization()(conv5)
    conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(conv5)
    conv5 = BatchNormalization()(conv5)
    conv5 = Dropout(0.5)(conv5)

    # Decoder with Attention
    g = conv5  # 디코더의 현재 단계에서의 특징 맵(가장 깊은 층에서의 컨볼루션 결과인 conv5를 사용)
    x = conv4  # 해당 디코더 단계에 대응하는 인코더 층의 특징 맵
    attended_conv4 = attention_block(x, g, 256)  # 어텐션 적용

    up6 = concatenate([Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(g), attended_conv4], axis=-1)
    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(up6)
    conv6 = BatchNormalization()(conv6)
    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv6)
    conv6 = BatchNormalization()(conv6)

    up7 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=-1)
    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(up7)
    conv7 = BatchNormalization()(conv7)
    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv7)
    conv7 = BatchNormalization()(conv7)

    up8 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=-1)
    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(up8)
    conv8 = BatchNormalization()(conv8)
    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)
    conv8 = BatchNormalization()(conv8)

    up9 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=-1)
    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(up9)
    conv9 = BatchNormalization()(conv9)
    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv9)
    conv9 = BatchNormalization()(conv9)

    conv10 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)
    conv10 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv10)

    # Output
    outputs = Conv2D(4, (1, 1), activation='softmax')(conv10)
    model = Model(inputs=[inputs], outputs=outputs) # 모델 구성
    
    # 모델 컴파일
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',  # 수정된 부분
                  metrics=['accuracy'])
    
    return model
